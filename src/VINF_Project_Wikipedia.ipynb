{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhlukovanie článkov Wikipédie do kategórií na základe ich vedecko-spoločenskej oblasti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vypracoval:** Tomáš Babjak\n",
    "\n",
    "**Predmet:** Vyhľadávanie informácii\n",
    "\n",
    "**GitHub:** https://github.com/tomasbabjak/VINF_Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potrebne kniznice na importovanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "import re\n",
    "import datamuse\n",
    "import nltk\n",
    "import json\n",
    "import string\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import time\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vytvoriť testovaciu vzorku dát, na ktorej budeme prvotne projekt realizovať"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read XML file with Wiki articles and parse articles to list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(file_name, n_first_articles):\n",
    "    \n",
    "    start_tag = f'<page>'\n",
    "    end_tag = f'</page>'\n",
    "    \n",
    "    start_found = False\n",
    "    articles_found = []\n",
    "    lines = ''\n",
    "    \n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if start_tag in line:\n",
    "                start_found = True\n",
    "            if start_found:\n",
    "                lines += line\n",
    "            if end_tag in line:\n",
    "                start_found = False\n",
    "                articles_found.append(lines)\n",
    "                lines = ''\n",
    "            if len(articles_found) == n_first_articles:\n",
    "                break\n",
    "    with open(f'../data/wiki_{n_first_articles}_before.json', 'w') as outfile:\n",
    "        json.dump(articles_found, outfile, indent=4)\n",
    "    return articles_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml_modified(file_name, n_first_articles):\n",
    "    \n",
    "    start_tag = f'<page>'\n",
    "    end_tag = f'</page>'\n",
    "    \n",
    "    start_found = False\n",
    "    articles_found = []\n",
    "    lines = ''\n",
    "    counter = 0\n",
    "    \n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if start_tag in line:\n",
    "                start_found = True\n",
    "            if start_found:\n",
    "                lines += line\n",
    "            if end_tag in line:\n",
    "                start_found = False\n",
    "                articles_found.append(lines)\n",
    "                lines = ''\n",
    "            if len(articles_found) == n_first_articles:\n",
    "                counter += 1\n",
    "                print(counter)\n",
    "                with open(f'../data/wiki_{counter}_before.json', 'w') as outfile:\n",
    "                    json.dump(articles_found, outfile, indent=4)\n",
    "                articles_found = []\n",
    "#             if counter == 10:\n",
    "#                 break\n",
    "    return articles_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Title and Text attributes from article and create dictionary from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(text):\n",
    "    title_regex = r'<title[^>]*>([^<]+)</title>'\n",
    "    text_regex = r'<text[^>]*>([^<]+)</text>'\n",
    "    pages = []\n",
    "    for page in text:\n",
    "        title = regex.findall(title_regex, page)\n",
    "        text = regex.findall(text_regex, page)\n",
    "        pages.append({\"title\": title[0] if title else '',\n",
    "                      \"text\": text[0] if text else ''})\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Najst infobox, anchor texty a wiki kategorie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Z článkov testovacej sady vyhľadať dôležité pojmy - zamerať sa na Infobox, kde sa nachádzajú dôležité informácie o článku\n",
    "\n",
    "### 5. Vyhľadať odkazy na iné články Wikipédie (anchor text), ktoré môžu smerovať priamo na oblasť alebo aspoň priblížiť kontext článku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Infobox and Achor texts from Text attribute of article and add them to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_infobox_anchor(text):\n",
    "    regex_infobox = r\"(?=\\{Infobox )(\\{([^{}]|(?1))*\\})\"\n",
    "    regex_anchor = r\"\\[\\[([^\\]\\[:]+)\\|([^\\]\\[:]+)\\]\\]\"\n",
    "    regex_category = r\"\\[\\[Category:([^\\]]*\\b)\"\n",
    "    for page in text:\n",
    "        page['infobox'] = regex.findall(regex_infobox, page['text'])\n",
    "        page['anchors'] = regex.findall(regex_anchor, page['text'])\n",
    "        page['category_wiki'] = regex.findall(regex_category, page['text'])\n",
    "        page['text'] = regex.sub(regex_infobox, '', page['text'])\n",
    "        page['text'] = regex.sub(regex_anchor, '', page['text'])\n",
    "        page['text'] = regex.sub(regex_category, '', page['text'])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate Redirect articles from others into two separate lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_redirect(text):\n",
    "    regex_redirect = r\"^#redirect[^\\[]*\\[\\[([^\\]]+)\"\n",
    "    redirect_pages = []\n",
    "    article_pages = []\n",
    "    for page in text:\n",
    "        if regex.findall(regex_redirect, page['text']):\n",
    "            redirect_pages.append(page)\n",
    "        else:\n",
    "            article_pages.append(page)\n",
    "    return (redirect_pages, article_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vytvoriť zoznam (strom) spoločensko-vedných oblastí, do ktorých budeme jednotlivé stránky zaraďovať, ku každej oblasti nájsť aj slová, ktoré sa s ňou spájajú"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find terms related to our categories with Datamuse library. Split words of each category and find 100 terms related to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_categories = [\n",
    "    'Culture',\n",
    "    'Food',\n",
    "    'Language',\n",
    "    'Literature',\n",
    "    'Art',\n",
    "    'Dance',\n",
    "    'Film',\n",
    "    'Music',\n",
    "    'Theatre',\n",
    "    'Architecture',\n",
    "    'Painting',\n",
    "    'Sculpture',\n",
    "    'Games',\n",
    "    'Sport',\n",
    "    'Recreation',\n",
    "    'Media',\n",
    "    'Internet',\n",
    "    'Geography',\n",
    "    'Earth',\n",
    "    'Health',\n",
    "    'Fitness',\n",
    "    'Exercise',\n",
    "    'Life',\n",
    "    'Medicine',\n",
    "    'History',\n",
    "    'Education',\n",
    "    'Crime',\n",
    "    'War',\n",
    "    'Transport',\n",
    "    'Mathematics',\n",
    "    'Logic',\n",
    "    'Statistics',\n",
    "    'Biology',\n",
    "    'Nature',\n",
    "    'Science',\n",
    "    'Philosophy',\n",
    "    'Religion',\n",
    "    'Belief',\n",
    "    'Society',\n",
    "    'Technology',\n",
    "    'Computing',\n",
    "    'Electronics',\n",
    "    'Engineering']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vytvorit gazeteer pomocou Wiki clankov mojich kategorii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ku kazdej z mojich kategorii najst clanok wikipedie s rovnakym nazvom a pomocou neho neskor vytvorit gazeteer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_categories_articles(file_name):\n",
    "    start_tag = f'<page>'\n",
    "    end_tag = f'</page>'\n",
    "    title_regex = r'<title[^>]*>([^<]+)</title>'\n",
    "\n",
    "    start_found = False\n",
    "    reading = False\n",
    "    start_just = False\n",
    "    articles_found = []\n",
    "    lines = ''\n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if start_tag in line:\n",
    "                start_found = True\n",
    "                start_just = True\n",
    "                continue\n",
    "            if start_just:\n",
    "                category = regex.findall(title_regex, line)\n",
    "                if category[0] in new_categories:\n",
    "                    print(category[0])\n",
    "                    reading = True\n",
    "                start_just = False\n",
    "            if start_found and reading:\n",
    "                lines += line\n",
    "            if end_tag in line:\n",
    "                start_found = False\n",
    "                reading = False\n",
    "                if category[0] in new_categories:\n",
    "                    articles_found.append(lines)\n",
    "                category = ''\n",
    "                lines = ''\n",
    "            if len(articles_found) == len(new_categories):\n",
    "                break\n",
    "    with open(f'../data/wiki_categories.json', 'w') as outfile:\n",
    "        json.dump(articles_found, outfile)\n",
    "    return articles_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vytvorit gazeteer pomocou Datamuse kniznice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ku kazdej z mojich kategorii najst gazeteer pomocou kniznice Datamuse - related words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api = datamuse.Datamuse()\n",
    "        \n",
    "def categories_find_related(categories):\n",
    "    cats_with_words = []\n",
    "\n",
    "    for c in categories:\n",
    "        api_words = api.words(ml=c, max=20)\n",
    "        result = list(map(lambda x: x.get('word'), api_words))\n",
    "        result.append(c.lower())\n",
    "        cats_with_words.append({'category':c,'related_words':result})\n",
    "\n",
    "    return cats_with_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predspracovanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Články vhodne predspracovať - stemming, tokenizácia, odstránenie stop slov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    text_tokens = [token.lower() for token in text_tokens if token not in [\"*+'-./:;,|<=>?@[\\]^_`{}~!\\\"#$%&()\\n\"]]\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(text):\n",
    "    tokens_without_stops = list(filter(lambda x: (x not in string.punctuation) and (x not in stopwords.words('english')),text))\n",
    "    return tokens_without_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_list(llist):\n",
    "    return [stemmer.stem(word) for word in llist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    else:\n",
    "        text = tokenize_text(text)\n",
    "        text = remove_stops(text)\n",
    "        text = stem_list(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_train(train_set):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    docs_tfidf = vectorizer.fit_transform(train_set)\n",
    "    return vectorizer, docs_tfidf\n",
    "\n",
    "def tfdif_test_cosine(query, vectorizer, docs_tfidf):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
    "    categories_sims = {}\n",
    "    \n",
    "    for cosine, category in zip(cosineSimilarities, new_categories):\n",
    "        if cosine != 0:\n",
    "            categories_sims[category] = cosine\n",
    "    return categories_sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invertovany index\n",
    "Jednoduchý vlastný invertovaný index pomocou python Hash Mapy - defaultdict. \n",
    "\n",
    "Prvým parametrom pri inicializacií Indexu sú všetky články z ktorých sa tvorí tento index a druhým je nejaký stĺpec podľa ktorého sa bude dať vyhľadávať článok.\n",
    "\n",
    "Pri hľadaní sa v prvom parametri uvádzajú slová pre vyhľadávanie oddelené medzerou a druhým parametrom je buď uvedené slovo \"and\" alebo \"or\" podľa toho či chceme aby sa obe tieto slová nachádzali v hľadanom texte alebo aspoň jedno z nich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class invertedIndex(object):\n",
    "\n",
    "    def __init__(self,docs,method):\n",
    "        self.docSets = defaultdict(set)\n",
    "        for doc in docs:\n",
    "            index = doc.get('title')\n",
    "            t = [preprocess_text(a) for a in doc.get(method)]\n",
    "            for term in [item for sublist in t for item in sublist]:\n",
    "                self.docSets[term].add(index)\n",
    "        #print(self.docSets)\n",
    "        \n",
    "    def search(self, term, andor):\n",
    "        pole=set()            \n",
    "        for a in preprocess_text(term):\n",
    "            #print(self.docSets[a])\n",
    "            if andor == 'and':\n",
    "                if len(pole) == 0:\n",
    "                    pole = self.docSets[a]\n",
    "                else:\n",
    "                    pole = pole.intersection(self.docSets[a])\n",
    "            elif andor == 'or':\n",
    "                pole = pole.union(self.docSets[a])\n",
    "        return pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spustenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre všetky spustenia je potrebné nájsť Wiki články kategórií:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_articles = find_categories_articles('../data/enwiki-latest-pages-articles.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predspracovat slova pre gazeteer - WIKI clanky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_articles = find_infobox_anchor(extract_text(categories_articles))\n",
    "for art in categories_articles:\n",
    "    art['text_tokens'] = preprocess_text(art.get('text'))# num = 0\n",
    "    art['category_wiki_tokens'] = preprocess_text(' '.join(art.get('category_wiki')))\n",
    "    if art.get('infobox'):\n",
    "        art['infobox_tokens'] = preprocess_text(' '.join(art.get('infobox')[0]))\n",
    "    else:\n",
    "        art['infobox_tokens'] = []\n",
    "    art['anchors_tokens'] = preprocess_text(' '.join([' '.join(tups) for tups in art.get('anchors')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predspracovat slova pre gazeteer - DATAMUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_with_words = categories_find_related(new_categories)\n",
    "for cat in cats_with_words:\n",
    "    cat['related_tokens'] = preprocess_text(' '.join(cat.get('related_words')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  TF-IDF a kosinusova podobnost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natrenovať Term frequency - Inverse document frequency na datasete gazeteeru, teda slov jednotlivych kategorii.\n",
    "\n",
    "Tento model vyhodnotime na slovach textu clankov, infoboxov, kategorii a anchor textoch a vypocitame kosinusovu podobnost s kategoriami gazeteeru."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trenovanie TF-IDF na gazeteere z DATAMUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = [' '.join(cat.get('related_tokens')) for cat in cats_with_words]\n",
    "\n",
    "vectorizer_datamuse, trained_model_datamuse = tfidf_train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trenovanie TF-IDF na gazeteere z WIKI clankov kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_text = [' '.join(cat.get('text_tokens')) for cat in categories_articles]\n",
    "train_set_category = [' '.join(cat.get('category_wiki_tokens')) for cat in categories_articles]\n",
    "train_set_infobox = [' '.join(cat.get('infobox_tokens')) for cat in categories_articles]\n",
    "train_set_anchors = [' '.join(cat.get('anchors_tokens')) for cat in categories_articles]\n",
    "\n",
    "vectorizer_wiki0, trained_model_wiki0 = tfidf_train(train_set_anchors)\n",
    "vectorizer_wiki1, trained_model_wiki1 = tfidf_train(train_set_infobox)\n",
    "vectorizer_wiki2, trained_model_wiki2 = tfidf_train(train_set_category)\n",
    "vectorizer_wiki3, trained_model_wiki3 = tfidf_train(train_set_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spustenie na testovacej vzorke dát - 30 článkov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Načítanie článkov, parsovanie potrebných častí článku a oddelenie redirectov od článkov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects, articles_test = find_redirect(find_infobox_anchor(extract_text(read_xml_modified('../data/enwiki-latest-pages-articles.xml', 50))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predspracovanie slov textu clanku, kategorii, infoboxov a anchor textov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for art in articles_test:\n",
    "    # Predspracovat slova textu clanku:\n",
    "    art['text_tokens'] = preprocess_text(art.get('text'))\n",
    "    # Predspracovat slova z kategorii:\n",
    "    art['category_wiki_tokens'] = preprocess_text(' '.join(art.get('category_wiki')))\n",
    "    # Predspracovat slova z infoboxov:\n",
    "    if art.get('infobox'):\n",
    "        art['infobox_tokens'] = preprocess_text(' '.join(art.get('infobox')[0]))\n",
    "    else:\n",
    "        art['infobox_tokens'] = []\n",
    "    # Predspracovat slova z anchor textov:\n",
    "    art['anchors_tokens'] = preprocess_text(' '.join([' '.join(tups) for tups in art.get('anchors')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kosinusova podobnost s testovacimi clankami Wiki - DATAMUSE TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for art in articles_test:\n",
    "    art['anchor_sims'] = tfdif_test_cosine(' '.join(art.get('anchors_tokens')), vectorizer_datamuse, trained_model_datamuse)\n",
    "    art['anchor_sims'] = {k: v for k, v in sorted(art.get('anchor_sims').items(), key = lambda item: item[1], reverse=True)}\n",
    "    art['categories_sims'] = tfdif_test_cosine(' '.join(art.get('category_wiki_tokens')), vectorizer_datamuse, trained_model_datamuse)\n",
    "    art['categories_sims'] = {k: v for k, v in sorted(art.get('categories_sims').items(), key = lambda item: item[1], reverse=True)}        \n",
    "    art['infobox_sims'] = tfdif_test_cosine(' '.join(art.get('infobox_tokens')), vectorizer_datamuse, trained_model_datamuse)\n",
    "    art['infobox_sims'] = {k: v for k, v in sorted(art.get('infobox_sims').items(), key = lambda item: item[1], reverse=True)}    \n",
    "    art['text_sims'] = tfdif_test_cosine(' '.join(art.get('text_tokens')), vectorizer_datamuse, trained_model_datamuse)\n",
    "    art['text_sims'] = {k: v for k, v in sorted(art.get('text_sims').items(), key = lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kosinusova podobnost s  s testovacimi clankami - WIKI Infobox TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for art in articles_test:\n",
    "    art['anchor_sims_info'] = tfdif_test_cosine(' '.join(art.get('anchors_tokens')), vectorizer_wiki1, trained_model_wiki1)\n",
    "    art['anchor_sims_info'] = {k: v for k, v in sorted(art.get('anchor_sims_info').items(), key = lambda item: item[1], reverse=True)}    \n",
    "    art['categories_sims_info'] = tfdif_test_cosine(' '.join(art.get('category_wiki_tokens')), vectorizer_wiki1, trained_model_wiki1)\n",
    "    art['categories_sims_info'] = {k: v for k, v in sorted(art.get('categories_sims_info').items(), key = lambda item: item[1], reverse=True)}    \n",
    "    art['infobox_sims_info'] = tfdif_test_cosine(' '.join(art.get('infobox_tokens')), vectorizer_wiki1, trained_model_wiki1)\n",
    "    art['infobox_sims_info'] = {k: v for k, v in sorted(art.get('infobox_sims_info').items(), key = lambda item: item[1], reverse=True)}\n",
    "    art['text_sims_info'] = tfdif_test_cosine(' '.join(art.get('text_tokens')), vectorizer_wiki1, trained_model_wiki1)\n",
    "    art['text_sims_info'] = {k: v for k, v in sorted(art.get('text_sims_info').items(), key = lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kosinusova podobnost  s testovacimi clankami - WIKI kategorie TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for art in articles_test:\n",
    "    art['anchor_sims_cat'] = tfdif_test_cosine(' '.join(art.get('anchors_tokens')), vectorizer_wiki2, trained_model_wiki2)\n",
    "    art['anchor_sims_cat'] = {k: v for k, v in sorted(art.get('anchor_sims_cat').items(), key = lambda item: item[1], reverse=True)}    \n",
    "    art['categories_sims_cat'] = tfdif_test_cosine(' '.join(art.get('category_wiki_tokens')), vectorizer_wiki2, trained_model_wiki2)\n",
    "    art['categories_sims_cat'] = {k: v for k, v in sorted(art.get('categories_sims_cat').items(), key = lambda item: item[1], reverse=True)}    \n",
    "    art['infobox_sims_cat'] = tfdif_test_cosine(' '.join(art.get('infobox_tokens')), vectorizer_wiki2, trained_model_wiki2)\n",
    "    art['infobox_sims_cat'] = {k: v for k, v in sorted(art.get('infobox_sims_cat').items(), key = lambda item: item[1], reverse=True)}\n",
    "    art['text_sims_cat'] = tfdif_test_cosine(' '.join(art.get('text_tokens')), vectorizer_wiki2, trained_model_wiki2)\n",
    "    art['text_sims_cat'] = {k: v for k, v in sorted(art.get('text_sims_cat').items(), key = lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kosinusova podobnost  s testovacimi clankami - WIKI text TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for art in articles_test:\n",
    "    art['anchor_sims_text'] = tfdif_test_cosine(' '.join(art.get('anchors_tokens')), vectorizer_wiki3, trained_model_wiki3)\n",
    "    art['anchor_sims_text'] = {k: v for k, v in sorted(art.get('anchor_sims_text').items(), key = lambda item: item[1], reverse=True)}    \n",
    "    art['categories_sims_text'] = tfdif_test_cosine(' '.join(art.get('category_wiki_tokens')), vectorizer_wiki3, trained_model_wiki3)\n",
    "    art['categories_sims_text'] = {k: v for k, v in sorted(art.get('categories_sims_text').items(), key = lambda item: item[1], reverse=True)}    \n",
    "    art['infobox_sims_text'] = tfdif_test_cosine(' '.join(art.get('infobox_tokens')), vectorizer_wiki3, trained_model_wiki3)\n",
    "    art['infobox_sims_text'] = {k: v for k, v in sorted(art.get('infobox_sims_text').items(), key = lambda item: item[1], reverse=True)}    \n",
    "    art['text_sims_text'] = tfdif_test_cosine(' '.join(art.get('text_tokens')), vectorizer_wiki3, trained_model_wiki3)\n",
    "    art['text_sims_text'] = {k: v for k, v in sorted(art.get('text_sims_text').items(), key = lambda item: item[1], reverse=True)}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spustenie na všetkých dátach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spustenie na všetkých dátach neodporúčame spúšťať ak nemáme cluster alebo stroj ktorý môže bežať veľmi dlho alebo je veľmi výkonný, tento proces môže trvať aj niekoľko dní."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "read_xml_modified('../data/enwiki-latest-pages-articles.xml', 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,207):\n",
    "    with open(f'../data/wiki_{i}_before.json', 'r+') as json_file:\n",
    "        print(i)\n",
    "        articles_all = json.load(json_file)\n",
    "        redirects, articles_all = find_redirect(find_infobox_anchor(extract_text(articles_all)))\n",
    "    with open(f'../data/wiki_{i}_before.json', 'w+') as json_file:\n",
    "        json.dump(articles_all, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,207):\n",
    "    start_time = time.time()\n",
    "    with open(f'../data/wiki_{i}_before.json', 'r+') as json_file:\n",
    "        articles_all = json.load(json_file)\n",
    "        for index, art in zip(range(len(articles_all)), articles_all):\n",
    "            if (index % 10000 == 1):\n",
    "                print(index)\n",
    "            art['text_tokens'] = preprocess_text(art.pop('text',''))\n",
    "            art['category_wiki_tokens'] = preprocess_text(' '.join(art.pop('category_wiki','')))\n",
    "            if art.get('infobox'):\n",
    "                art['infobox_tokens'] = preprocess_text(' '.join(art.pop('infobox','')[0]))\n",
    "            else:\n",
    "                art['infobox_tokens'] = []\n",
    "            art['anchors_tokens'] = preprocess_text(' '.join([' '.join(tups) for tups in art.pop('anchors','')]))\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time) / 60))\n",
    "    with open(f'../data/wiki_{i}_before.json', 'w+') as json_file:\n",
    "        json.dump(articles_all, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kosinusova podobnost s Kategorickymi clankami Wiki - DATAMUSE TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narozdiel od testovacich, vsetky clanky sme skusali iba na najuspesnejsom testovanom modely, teda pomocou gazeteera DATAMUSE.\n",
    "\n",
    "Podobnost s textom je zakomentovana kvoli jeho vyssej casovej narocnosti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16,64):\n",
    "    start_time = time.time()\n",
    "    with open(f'../data/wiki_{i}_before.json') as json_file:\n",
    "        more_articles = json.load(json_file)\n",
    "\n",
    "    for art in more_articles:\n",
    "        art['anchor_sims'] = tfdif_test_cosine(' '.join(art.get('anchors_tokens')), vectorizer_datamuse, trained_model_datamuse)\n",
    "        art['anchor_sims'] = {k: v for k, v in sorted(art.get('anchor_sims').items(), key = lambda item: item[1], reverse=True)}\n",
    "        art['categories_sims'] = tfdif_test_cosine(' '.join(art.get('category_wiki_tokens')), vectorizer_datamuse, trained_model_datamuse)\n",
    "        art['categories_sims'] = {k: v for k, v in sorted(art.get('categories_sims').items(), key = lambda item: item[1], reverse=True)}        \n",
    "        art['infobox_sims'] = tfdif_test_cosine(' '.join(art.get('infobox_tokens')), vectorizer_datamuse, trained_model_datamuse)\n",
    "        art['infobox_sims'] = {k: v for k, v in sorted(art.get('infobox_sims').items(), key = lambda item: item[1], reverse=True)}    \n",
    "#         art['text_sims'] = tfdif_test_cosine(' '.join(art.get('text_tokens')), vectorizer_datamuse, trained_model_datamuse)\n",
    "#         art['text_sims'] = {k: v for k, v in sorted(art.get('text_sims').items(), key = lambda item: item[1], reverse=True)}\n",
    "\n",
    "    with open(f'../data/wiki_{i}_after.json', 'w') as outfile:\n",
    "        json.dump(more_articles, outfile, indent=4)\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testovanie a vyhodnotenie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako funguje testovanie a vyhodnocovanie?\n",
    "\n",
    "Nadpis testovania hovorí o gazeteere, na ktorom bol trénovaný TFIDF model.\n",
    "\n",
    "Pomocou tohto modelu sa vyhodnocuje kosínusová podobnosť kategórií ku všetkým 4 častiam článku.\n",
    "\n",
    "Následne pre každý z testovaných článkov zistím koľko anotovaných kategórií bolo priradených anotátorom.\n",
    "\n",
    "Pre každú zo 4 častí článku zoberiem práve toľko zistených kategórií, koľko bolo anotovaných a spravím zjednotenie týchto kategórií -> stlpec ALL.\n",
    "\n",
    "Pre každú zo 4 častí + zjednotenia vyhodnotím úspešnosť nájdených kategórií vzhľadom k anotovaným kategóriám v percentách.\n",
    "\n",
    "Následne vypočítam priemer zo všetkých článkov a ďalšie štatistiky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATAMUSE gazeteer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor text +  Kategorie Wiki + Infobox + Text clanku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Anchor  Categories  Infobox    Text  \\\n",
      "Anarchism                                  25.00       75.00     0.00   50.00   \n",
      "Autism                                     66.67        0.00    33.33    0.00   \n",
      "Albedo                                      0.00        0.00     0.00    0.00   \n",
      "A                                          50.00        0.00    50.00    0.00   \n",
      "Alabama                                     0.00        0.00     0.00    0.00   \n",
      "Achilles                                    0.00       66.67     0.00   66.67   \n",
      "Abraham Lincoln                            40.00       40.00    20.00   20.00   \n",
      "Aristotle                                  57.14       57.14    57.14   42.86   \n",
      "An American in Paris                       50.00       25.00     0.00   50.00   \n",
      "Academy Award for Best Production Design   66.67       66.67    66.67   66.67   \n",
      "Academy Awards                             66.67       66.67    66.67   66.67   \n",
      "Actrius                                     0.00      100.00   100.00  100.00   \n",
      "Animalia (book)                           100.00      100.00   100.00  100.00   \n",
      "International Atomic Time                   0.00        0.00     0.00    0.00   \n",
      "Altruism                                   33.33       66.67     0.00   33.33   \n",
      "Ayn Rand                                   33.33       33.33     0.00   66.67   \n",
      "Alain Connes                               33.33       33.33    66.67   33.33   \n",
      "Allan Dwan                                100.00      100.00   100.00  100.00   \n",
      "Algeria                                     0.00        0.00     0.00    0.00   \n",
      "List of Atlas Shrugged characters           0.00      100.00     0.00  100.00   \n",
      "Anthropology                               40.00        0.00     0.00   60.00   \n",
      "Agricultural science                       50.00       25.00     0.00   75.00   \n",
      "Alchemy                                    50.00        0.00     0.00    0.00   \n",
      "Alien                                      25.00        0.00     0.00   25.00   \n",
      "Astronomer                                 66.67       66.67     0.00   33.33   \n",
      "ASCII                                      50.00       50.00     0.00   25.00   \n",
      "Austin (disambiguation)                     0.00        0.00     0.00    0.00   \n",
      "Animation                                  66.67       66.67     0.00   33.33   \n",
      "Apollo                                      0.00        0.00     0.00   33.33   \n",
      "Andre Agassi                                0.00        0.00     0.00   50.00   \n",
      "\n",
      "                                             All  \n",
      "Anarchism                                  75.00  \n",
      "Autism                                     66.67  \n",
      "Albedo                                      0.00  \n",
      "A                                          50.00  \n",
      "Alabama                                     0.00  \n",
      "Achilles                                   66.67  \n",
      "Abraham Lincoln                            60.00  \n",
      "Aristotle                                  85.71  \n",
      "An American in Paris                       50.00  \n",
      "Academy Award for Best Production Design  100.00  \n",
      "Academy Awards                             66.67  \n",
      "Actrius                                   100.00  \n",
      "Animalia (book)                           100.00  \n",
      "International Atomic Time                   0.00  \n",
      "Altruism                                   83.33  \n",
      "Ayn Rand                                   66.67  \n",
      "Alain Connes                               66.67  \n",
      "Allan Dwan                                100.00  \n",
      "Algeria                                     0.00  \n",
      "List of Atlas Shrugged characters         100.00  \n",
      "Anthropology                               60.00  \n",
      "Agricultural science                      100.00  \n",
      "Alchemy                                    50.00  \n",
      "Alien                                      25.00  \n",
      "Astronomer                                100.00  \n",
      "ASCII                                      50.00  \n",
      "Austin (disambiguation)                     0.00  \n",
      "Animation                                 100.00  \n",
      "Apollo                                     33.33  \n",
      "Andre Agassi                               50.00  \n",
      "           Anchor  Categories     Infobox        Text         All\n",
      "count   30.000000   30.000000   30.000000   30.000000   30.000000\n",
      "mean    35.682667   37.960667   22.016000   41.039667   60.190667\n",
      "std     30.835777   37.092951   35.267161   33.742612   34.684756\n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000\n",
      "25%      0.000000    0.000000    0.000000    5.000000   50.000000\n",
      "50%     36.665000   33.330000    0.000000   33.330000   66.670000\n",
      "75%     55.355000   66.670000   45.832500   66.670000   96.427500\n",
      "max    100.000000  100.000000  100.000000  100.000000  100.000000\n",
      "\n",
      "Average of All column:  60.19\n",
      "Precision:  0.48585954275609444\n",
      "Recall:  0.6436781609195402\n",
      "F1: 0.5077648405234612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:994: UserWarning: unknown class(es) ['Crime', 'Dance', 'Earth', 'Education', 'Exercise', 'Fitness', 'Food', 'Internet', 'Media', 'Painting'] will be ignored\n",
      "  warnings.warn('unknown class(es) {0} will be ignored'\n"
     ]
    }
   ],
   "source": [
    "from more_itertools import take\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "with open('../data/test_30_tested.json') as json_file:\n",
    "    articles_test = json.load(json_file)\n",
    "    \n",
    "titles = [art.get('title') for art in articles_test]\n",
    "anchors = [0] * 30\n",
    "categories = [0] * 30\n",
    "infoboxes = [0] * 30\n",
    "texts = [0] * 30\n",
    "extended = [0] * 30\n",
    "counter = 0\n",
    "\n",
    "for art in articles_test:\n",
    "    len_annotated = len(art.get('annotated_categories'))\n",
    "    \n",
    "    n_anchors = take(len_annotated, art.get('anchor_sims').keys())\n",
    "    n_anchors.extend(take(len_annotated, art.get('categories_sims').keys()))\n",
    "    n_anchors.extend(take(len_annotated, art.get('infobox_sims').keys()))\n",
    "    n_anchors.extend(take(len_annotated, art.get('text_sims').keys()))\n",
    "    art['extended_sims'] = list(set(n_anchors))\n",
    "    \n",
    "    for cat in art.get('annotated_categories'):\n",
    "        if cat in art.get('anchor_sims') and not pd.isna(art.get('anchor_sims').get(cat)) and list(art.get('anchor_sims')).index(cat) + 1 <= len_annotated:\n",
    "            anchors[counter] += 1\n",
    "        if cat in art.get('categories_sims') and not pd.isna(art.get('categories_sims').get(cat)) and list(art.get('categories_sims')).index(cat) + 1 <= len_annotated:\n",
    "            categories[counter] += 1\n",
    "        if cat in art.get('infobox_sims') and not pd.isna(art.get('infobox_sims').get(cat)) and list(art.get('infobox_sims')).index(cat) + 1 <= len_annotated:\n",
    "            infoboxes[counter] += 1\n",
    "        if cat in art.get('text_sims') and not pd.isna(art.get('text_sims').get(cat)) and list(art.get('text_sims')).index(cat) + 1 <= len_annotated:\n",
    "            texts[counter] += 1\n",
    "        if cat in art.get('extended_sims'):\n",
    "            extended[counter] += 1\n",
    "    anchors[counter] = float(\"{:.2f}\".format(anchors[counter] / len_annotated * 100))\n",
    "    categories[counter] = float(\"{:.2f}\".format(categories[counter] / len_annotated * 100))\n",
    "    infoboxes[counter] = float(\"{:.2f}\".format(infoboxes[counter] / len_annotated * 100))\n",
    "    texts[counter] = float(\"{:.2f}\".format(texts[counter] / len_annotated* 100))\n",
    "    extended[counter] = float(\"{:.2f}\".format(extended[counter] / len_annotated* 100))\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "cars = {\n",
    "    'Anchor': anchors,\n",
    "    'Categories': categories,\n",
    "    'Infobox': infoboxes, \n",
    "    'Text': texts,\n",
    "    'All': extended\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars, columns = ['Anchor','Categories','Infobox','Text','All'], index=titles)\n",
    "\n",
    "print(df)\n",
    "print (df.describe())\n",
    "print('\\nAverage of All column: ', float(\"{:.2f}\".format(df['All'].mean())))\n",
    "\n",
    "A=[art['annotated_categories'] for art in articles_test]\n",
    "B=[art['extended_sims'] for art in articles_test]\n",
    "#A=[[ \"Culture\",\"Philosophy\",\"Belief\",\"Society\"],['Society']]\n",
    "#B=[['Culture', 'Philosophy', 'Belief','dsdasd'],['dsd']]\n",
    "\n",
    "multi = MultiLabelBinarizer()\n",
    "\n",
    "y_true = multi.fit(A).transform(A)\n",
    "y_pred = multi.transform(B)\n",
    "\n",
    "print('Precision: ',precision_score(y_true, y_pred,average='weighted',zero_division=1))\n",
    "print('Recall: ',recall_score(y_true, y_pred, average='weighted',zero_division=1))\n",
    "print('F1:' ,f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKI kategorie gazeteer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor text +  Kategorie Wiki + Infobox + Text clanku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Anchor  Categories  Infobox   Text  \\\n",
      "Anarchism                                   0.00       25.00     0.00  25.00   \n",
      "Autism                                      0.00        0.00     0.00   0.00   \n",
      "Albedo                                      0.00        0.00     0.00   0.00   \n",
      "A                                           0.00        0.00     0.00   0.00   \n",
      "Alabama                                     0.00        0.00     0.00   0.00   \n",
      "Achilles                                    0.00        0.00     0.00   0.00   \n",
      "Abraham Lincoln                             0.00        0.00     0.00   0.00   \n",
      "Aristotle                                  28.57        0.00    28.57  14.29   \n",
      "An American in Paris                        0.00        0.00     0.00   0.00   \n",
      "Academy Award for Best Production Design    0.00        0.00     0.00   0.00   \n",
      "Academy Awards                              0.00        0.00     0.00   0.00   \n",
      "Actrius                                     0.00        0.00     0.00   0.00   \n",
      "Animalia (book)                             0.00        0.00     0.00   0.00   \n",
      "International Atomic Time                   0.00        0.00     0.00   0.00   \n",
      "Altruism                                    0.00       33.33     0.00  16.67   \n",
      "Ayn Rand                                    0.00        0.00     0.00   0.00   \n",
      "Alain Connes                                0.00        0.00    33.33   0.00   \n",
      "Allan Dwan                                  0.00        0.00     0.00   0.00   \n",
      "Algeria                                     0.00        0.00     0.00   0.00   \n",
      "List of Atlas Shrugged characters           0.00        0.00     0.00   0.00   \n",
      "Anthropology                                0.00        0.00     0.00   0.00   \n",
      "Agricultural science                        0.00        0.00     0.00   0.00   \n",
      "Alchemy                                     0.00        0.00     0.00   0.00   \n",
      "Alien                                       0.00        0.00     0.00   0.00   \n",
      "Astronomer                                  0.00        0.00     0.00   0.00   \n",
      "ASCII                                       0.00        0.00     0.00   0.00   \n",
      "Austin (disambiguation)                     0.00        0.00     0.00   0.00   \n",
      "Animation                                  33.33       33.33     0.00  33.33   \n",
      "Apollo                                      0.00        0.00    33.33   0.00   \n",
      "Andre Agassi                                0.00        0.00     0.00   0.00   \n",
      "\n",
      "                                            All  \n",
      "Anarchism                                 25.00  \n",
      "Autism                                     0.00  \n",
      "Albedo                                     0.00  \n",
      "A                                          0.00  \n",
      "Alabama                                    0.00  \n",
      "Achilles                                   0.00  \n",
      "Abraham Lincoln                            0.00  \n",
      "Aristotle                                 42.86  \n",
      "An American in Paris                       0.00  \n",
      "Academy Award for Best Production Design   0.00  \n",
      "Academy Awards                             0.00  \n",
      "Actrius                                    0.00  \n",
      "Animalia (book)                            0.00  \n",
      "International Atomic Time                  0.00  \n",
      "Altruism                                  33.33  \n",
      "Ayn Rand                                   0.00  \n",
      "Alain Connes                              33.33  \n",
      "Allan Dwan                                 0.00  \n",
      "Algeria                                    0.00  \n",
      "List of Atlas Shrugged characters          0.00  \n",
      "Anthropology                               0.00  \n",
      "Agricultural science                       0.00  \n",
      "Alchemy                                    0.00  \n",
      "Alien                                      0.00  \n",
      "Astronomer                                 0.00  \n",
      "ASCII                                      0.00  \n",
      "Austin (disambiguation)                    0.00  \n",
      "Animation                                 33.33  \n",
      "Apollo                                    33.33  \n",
      "Andre Agassi                               0.00  \n",
      "          Anchor  Categories    Infobox       Text        All\n",
      "count  30.000000   30.000000  30.000000  30.000000  30.000000\n",
      "mean    2.063333    3.055333   3.174333   2.976333   6.706000\n",
      "std     7.877102    9.407858   9.712649   8.204761  13.841993\n",
      "min     0.000000    0.000000   0.000000   0.000000   0.000000\n",
      "25%     0.000000    0.000000   0.000000   0.000000   0.000000\n",
      "50%     0.000000    0.000000   0.000000   0.000000   0.000000\n",
      "75%     0.000000    0.000000   0.000000   0.000000   0.000000\n",
      "max    33.330000   33.330000  33.330000  33.330000  42.860000\n",
      "\n",
      "Average of All column:  6.71\n",
      "Precision:  0.11683087027914613\n",
      "Recall:  0.10344827586206896\n",
      "F1: 0.08149972632731255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:994: UserWarning: unknown class(es) ['Crime', 'Dance', 'Earth', 'Exercise', 'Fitness', 'Food', 'Internet', 'Media', 'Painting', 'Recreation'] will be ignored\n",
      "  warnings.warn('unknown class(es) {0} will be ignored'\n"
     ]
    }
   ],
   "source": [
    "with open('../data/test_30_tested.json') as json_file:\n",
    "    articles_test = json.load(json_file)\n",
    "    \n",
    "titles = [art.get('title') for art in articles_test]\n",
    "anchors = [0] * 30\n",
    "categories = [0] * 30\n",
    "infoboxes = [0] * 30\n",
    "texts = [0] * 30\n",
    "extended = [0] * 30\n",
    "counter = 0\n",
    "\n",
    "for art in articles_test:\n",
    "    len_annotated = len(art.get('annotated_categories'))\n",
    "\n",
    "    n_anchors = take(len_annotated, art.get('anchor_sims_cat').keys())\n",
    "    n_anchors.extend(take(len_annotated, art.get('categories_sims_cat').keys()))\n",
    "    n_anchors.extend(take(len_annotated, art.get('infobox_sims_cat').keys()))\n",
    "    n_anchors.extend(take(len_annotated, art.get('text_sims_cat').keys()))\n",
    "    art['extended_sims_cat'] = list(set(n_anchors))\n",
    "\n",
    "    for cat in art.get('annotated_categories'):\n",
    "        if cat in art.get('anchor_sims_cat') and not pd.isna(art.get('anchor_sims_cat').get(cat)) and list(art.get('anchor_sims_cat')).index(cat) + 1 <= len_annotated:\n",
    "            anchors[counter] += 1\n",
    "            #print(cat, art.get('anchor_sims_cat').get(cat),list(art.get('anchor_sims_cat')).index(cat) + 1)\n",
    "        if cat in art.get('categories_sims_cat') and not pd.isna(art.get('categories_sims_cat').get(cat)) and list(art.get('categories_sims_cat')).index(cat) + 1 <= len_annotated:\n",
    "            categories[counter] += 1\n",
    "            #print(cat, art.get('categories_sims_cat').get(cat),list(art.get('categories_sims_cat')).index(cat) + 1)\n",
    "        if cat in art.get('infobox_sims_cat') and not pd.isna(art.get('infobox_sims_cat').get(cat)) and list(art.get('infobox_sims_cat')).index(cat) + 1 <= len_annotated:\n",
    "            infoboxes[counter] += 1\n",
    "            #print(cat, art.get('infobox_sims_cat').get(cat),list(art.get('infobox_sims_cat')).index(cat) + 1)\n",
    "        if cat in art.get('text_sims_cat') and not pd.isna(art.get('text_sims_cat').get(cat)) and list(art.get('text_sims_cat')).index(cat) + 1 <= len_annotated:\n",
    "            texts[counter] += 1\n",
    "            #print(cat, art.get('text_sims_cat').get(cat),list(art.get('text_sims_cat')).index(cat) + 1)\n",
    "        if cat in art.get('extended_sims_cat'):\n",
    "            extended[counter] += 1\n",
    "    anchors[counter] = float(\"{:.2f}\".format(anchors[counter] / len_annotated * 100))\n",
    "    categories[counter] = float(\"{:.2f}\".format(categories[counter] / len_annotated * 100))\n",
    "    infoboxes[counter] = float(\"{:.2f}\".format(infoboxes[counter] / len_annotated * 100))\n",
    "    texts[counter] = float(\"{:.2f}\".format(texts[counter] / len_annotated* 100))\n",
    "    extended[counter] = float(\"{:.2f}\".format(extended[counter] / len_annotated* 100))\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "cars = {\n",
    "    'Anchor': anchors,\n",
    "    'Categories': categories,\n",
    "    'Infobox': infoboxes, \n",
    "    'Text': texts,\n",
    "    'All': extended\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars, columns = ['Anchor','Categories','Infobox','Text','All'], index=titles)\n",
    "\n",
    "print(df)\n",
    "print (df.describe())\n",
    "print('\\nAverage of All column: ', float(\"{:.2f}\".format(df['All'].mean())))\n",
    "\n",
    "A=[art['annotated_categories'] for art in articles_test]\n",
    "B=[art['extended_sims_cat'] for art in articles_test]\n",
    "\n",
    "multi = MultiLabelBinarizer()\n",
    "\n",
    "y_true = multi.fit(A).transform(A)\n",
    "y_pred = multi.transform(B)\n",
    "\n",
    "print('Precision: ',precision_score(y_true, y_pred,average='weighted',zero_division=1))\n",
    "print('Recall: ',recall_score(y_true, y_pred, average='weighted',zero_division=1))\n",
    "print('F1:' ,f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKI Infobox gazeteer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor text +  Kategorie Wiki + Infobox + Text clanku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Anchor  Categories  Infobox    Text  \\\n",
      "Anarchism                                   0.00         0.0     0.00   50.00   \n",
      "Autism                                      0.00         0.0    33.33    0.00   \n",
      "Albedo                                      0.00         0.0     0.00    0.00   \n",
      "A                                           0.00         0.0    50.00    0.00   \n",
      "Alabama                                     0.00         0.0     0.00    0.00   \n",
      "Achilles                                    0.00         0.0     0.00   66.67   \n",
      "Abraham Lincoln                             0.00         0.0    20.00   20.00   \n",
      "Aristotle                                   0.00         0.0    57.14   42.86   \n",
      "An American in Paris                       25.00         0.0     0.00   50.00   \n",
      "Academy Award for Best Production Design   33.33         0.0    66.67   66.67   \n",
      "Academy Awards                             33.33         0.0    66.67   66.67   \n",
      "Actrius                                     0.00         0.0   100.00  100.00   \n",
      "Animalia (book)                             0.00         0.0   100.00  100.00   \n",
      "International Atomic Time                   0.00         0.0     0.00    0.00   \n",
      "Altruism                                    0.00         0.0     0.00   33.33   \n",
      "Ayn Rand                                    0.00         0.0     0.00   66.67   \n",
      "Alain Connes                                0.00         0.0    66.67   33.33   \n",
      "Allan Dwan                                  0.00         0.0   100.00  100.00   \n",
      "Algeria                                     0.00         0.0     0.00    0.00   \n",
      "List of Atlas Shrugged characters           0.00         0.0     0.00  100.00   \n",
      "Anthropology                                0.00         0.0     0.00   60.00   \n",
      "Agricultural science                        0.00         0.0     0.00   75.00   \n",
      "Alchemy                                     0.00         0.0     0.00    0.00   \n",
      "Alien                                       0.00         0.0     0.00   25.00   \n",
      "Astronomer                                  0.00         0.0     0.00   33.33   \n",
      "ASCII                                       0.00         0.0     0.00   25.00   \n",
      "Austin (disambiguation)                     0.00         0.0     0.00    0.00   \n",
      "Animation                                   0.00         0.0     0.00   33.33   \n",
      "Apollo                                      0.00         0.0     0.00   33.33   \n",
      "Andre Agassi                                0.00         0.0     0.00   50.00   \n",
      "\n",
      "                                             All  \n",
      "Anarchism                                  50.00  \n",
      "Autism                                     33.33  \n",
      "Albedo                                      0.00  \n",
      "A                                          50.00  \n",
      "Alabama                                     0.00  \n",
      "Achilles                                   66.67  \n",
      "Abraham Lincoln                            20.00  \n",
      "Aristotle                                  71.43  \n",
      "An American in Paris                       75.00  \n",
      "Academy Award for Best Production Design  100.00  \n",
      "Academy Awards                            100.00  \n",
      "Actrius                                   100.00  \n",
      "Animalia (book)                           100.00  \n",
      "International Atomic Time                   0.00  \n",
      "Altruism                                   33.33  \n",
      "Ayn Rand                                   66.67  \n",
      "Alain Connes                               66.67  \n",
      "Allan Dwan                                100.00  \n",
      "Algeria                                     0.00  \n",
      "List of Atlas Shrugged characters         100.00  \n",
      "Anthropology                               60.00  \n",
      "Agricultural science                       75.00  \n",
      "Alchemy                                     0.00  \n",
      "Alien                                      25.00  \n",
      "Astronomer                                 33.33  \n",
      "ASCII                                      25.00  \n",
      "Austin (disambiguation)                     0.00  \n",
      "Animation                                  33.33  \n",
      "Apollo                                     33.33  \n",
      "Andre Agassi                               50.00  \n",
      "          Anchor  Categories     Infobox        Text         All\n",
      "count  30.000000        30.0   30.000000   30.000000   30.000000\n",
      "mean    3.055333         0.0   22.016000   41.039667   48.936333\n",
      "std     9.407858         0.0   35.267161   33.742612   35.305765\n",
      "min     0.000000         0.0    0.000000    0.000000    0.000000\n",
      "25%     0.000000         0.0    0.000000    5.000000   25.000000\n",
      "50%     0.000000         0.0    0.000000   33.330000   50.000000\n",
      "75%     0.000000         0.0   45.832500   66.670000   74.107500\n",
      "max    33.330000         0.0  100.000000  100.000000  100.000000\n",
      "\n",
      "Average of All column:  48.94\n",
      "Precision:  0.5852490421455939\n",
      "Recall:  0.4942528735632184\n",
      "F1: 0.4698716765457588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:994: UserWarning: unknown class(es) ['Crime', 'Dance', 'Earth', 'Exercise', 'Fitness', 'Internet', 'Media', 'Painting', 'Recreation'] will be ignored\n",
      "  warnings.warn('unknown class(es) {0} will be ignored'\n"
     ]
    }
   ],
   "source": [
    "with open('../data/test_30_tested.json') as json_file:\n",
    "    articles_test = json.load(json_file)\n",
    "    \n",
    "titles = [art.get('title') for art in articles_test]\n",
    "anchors = [0] * 30\n",
    "categories = [0] * 30\n",
    "infoboxes = [0] * 30\n",
    "texts = [0] * 30\n",
    "extended = [0] * 30\n",
    "counter = 0\n",
    "\n",
    "for art in articles_test:\n",
    "    len_annotated = len(art.get('annotated_categories'))\n",
    "\n",
    "    n_anchors = take(len_annotated, art.get('anchor_sims_info').keys())\n",
    "    n_anchors.extend(take(len_annotated, art.get('categories_sims_info').keys()))\n",
    "    n_anchors.extend(take(len_annotated, art.get('infobox_sims_info').keys()))\n",
    "    n_anchors.extend(take(len_annotated, art.get('text_sims_info').keys()))\n",
    "    art['extended_sims_info'] = list(set(n_anchors))\n",
    "\n",
    "    for cat in art.get('annotated_categories'):\n",
    "        if cat in art.get('anchor_sims_info') and not pd.isna(art.get('anchor_sims_info').get(cat)) and list(art.get('anchor_sims_info')).index(cat) + 1 <= len_annotated:\n",
    "            anchors[counter] += 1\n",
    "        if cat in art.get('categories_sims_info') and not pd.isna(art.get('categories_sims_info').get(cat)) and list(art.get('categories_sims_info')).index(cat) + 1 <= len_annotated:\n",
    "            categories[counter] += 1\n",
    "        if cat in art.get('infobox_sims_info') and not pd.isna(art.get('infobox_sims_info').get(cat)) and list(art.get('infobox_sims_info')).index(cat) + 1 <= len_annotated:\n",
    "            infoboxes[counter] += 1\n",
    "        if cat in art.get('text_sims_info') and not pd.isna(art.get('text_sims_info').get(cat)) and list(art.get('text_sims_info')).index(cat) + 1 <= len_annotated:\n",
    "            texts[counter] += 1\n",
    "        if cat in art.get('extended_sims_info'):\n",
    "            extended[counter] += 1\n",
    "    anchors[counter] = float(\"{:.2f}\".format(anchors[counter] / len_annotated * 100))\n",
    "    categories[counter] = float(\"{:.2f}\".format(categories[counter] / len_annotated * 100))\n",
    "    infoboxes[counter] = float(\"{:.2f}\".format(infoboxes[counter] / len_annotated * 100))\n",
    "    texts[counter] = float(\"{:.2f}\".format(texts[counter] / len_annotated* 100))\n",
    "    extended[counter] = float(\"{:.2f}\".format(extended[counter] / len_annotated* 100))\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "cars = {\n",
    "    'Anchor': anchors,\n",
    "    'Categories': categories,\n",
    "    'Infobox': infoboxes, \n",
    "    'Text': texts,\n",
    "    'All': extended\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars, columns = ['Anchor','Categories','Infobox','Text','All'], index=titles)\n",
    "\n",
    "print(df)\n",
    "print (df.describe())\n",
    "print('\\nAverage of All column: ', float(\"{:.2f}\".format(df['All'].mean())))\n",
    "\n",
    "A=[art['annotated_categories'] for art in articles_test]\n",
    "B=[art['extended_sims_info'] for art in articles_test]\n",
    "\n",
    "multi = MultiLabelBinarizer()\n",
    "\n",
    "y_true = multi.fit(A).transform(A)\n",
    "y_pred = multi.transform(B)\n",
    "\n",
    "print('Precision: ',precision_score(y_true, y_pred,average='weighted',zero_division=1))\n",
    "print('Recall: ',recall_score(y_true, y_pred, average='weighted',zero_division=1))\n",
    "print('F1:' ,f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIKI Text gazeteer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor text +  Kategorie Wiki + Infobox + Text clanku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Anchor  Categories  Infobox   Text  \\\n",
      "Anarchism                                   0.00       25.00     0.00   0.00   \n",
      "Autism                                      0.00        0.00     0.00   0.00   \n",
      "Albedo                                      0.00        0.00     0.00   0.00   \n",
      "A                                           0.00        0.00     0.00   0.00   \n",
      "Alabama                                     0.00        0.00     0.00   0.00   \n",
      "Achilles                                    0.00        0.00     0.00   0.00   \n",
      "Abraham Lincoln                            20.00       40.00     0.00   0.00   \n",
      "Aristotle                                  14.29       14.29    14.29  14.29   \n",
      "An American in Paris                        0.00        0.00     0.00  25.00   \n",
      "Academy Award for Best Production Design    0.00        0.00    33.33  33.33   \n",
      "Academy Awards                              0.00        0.00     0.00   0.00   \n",
      "Actrius                                     0.00        0.00     0.00   0.00   \n",
      "Animalia (book)                             0.00        0.00     0.00   0.00   \n",
      "International Atomic Time                   0.00        0.00     0.00   0.00   \n",
      "Altruism                                    0.00        0.00     0.00  33.33   \n",
      "Ayn Rand                                    0.00        0.00     0.00   0.00   \n",
      "Alain Connes                                0.00        0.00     0.00   0.00   \n",
      "Allan Dwan                                  0.00        0.00     0.00   0.00   \n",
      "Algeria                                     0.00        0.00     0.00   0.00   \n",
      "List of Atlas Shrugged characters           0.00        0.00     0.00   0.00   \n",
      "Anthropology                               20.00       40.00     0.00  20.00   \n",
      "Agricultural science                        0.00        0.00     0.00   0.00   \n",
      "Alchemy                                     0.00        0.00     0.00   0.00   \n",
      "Alien                                       0.00        0.00     0.00   0.00   \n",
      "Astronomer                                  0.00        0.00     0.00   0.00   \n",
      "ASCII                                       0.00        0.00     0.00   0.00   \n",
      "Austin (disambiguation)                     0.00        0.00     0.00   0.00   \n",
      "Animation                                   0.00        0.00     0.00   0.00   \n",
      "Apollo                                      0.00        0.00     0.00   0.00   \n",
      "Andre Agassi                                0.00        0.00    50.00  50.00   \n",
      "\n",
      "                                            All  \n",
      "Anarchism                                 25.00  \n",
      "Autism                                     0.00  \n",
      "Albedo                                     0.00  \n",
      "A                                          0.00  \n",
      "Alabama                                    0.00  \n",
      "Achilles                                   0.00  \n",
      "Abraham Lincoln                           40.00  \n",
      "Aristotle                                 14.29  \n",
      "An American in Paris                      25.00  \n",
      "Academy Award for Best Production Design  33.33  \n",
      "Academy Awards                             0.00  \n",
      "Actrius                                    0.00  \n",
      "Animalia (book)                            0.00  \n",
      "International Atomic Time                  0.00  \n",
      "Altruism                                  33.33  \n",
      "Ayn Rand                                   0.00  \n",
      "Alain Connes                               0.00  \n",
      "Allan Dwan                                 0.00  \n",
      "Algeria                                    0.00  \n",
      "List of Atlas Shrugged characters          0.00  \n",
      "Anthropology                              40.00  \n",
      "Agricultural science                       0.00  \n",
      "Alchemy                                    0.00  \n",
      "Alien                                      0.00  \n",
      "Astronomer                                 0.00  \n",
      "ASCII                                      0.00  \n",
      "Austin (disambiguation)                    0.00  \n",
      "Animation                                  0.00  \n",
      "Apollo                                     0.00  \n",
      "Andre Agassi                              50.00  \n",
      "          Anchor  Categories    Infobox       Text        All\n",
      "count  30.000000   30.000000  30.000000  30.000000  30.000000\n",
      "mean    1.809667    3.976333   3.254000   5.865000   8.698333\n",
      "std     5.589267   11.071659  10.981862  13.023897  15.653745\n",
      "min     0.000000    0.000000   0.000000   0.000000   0.000000\n",
      "25%     0.000000    0.000000   0.000000   0.000000   0.000000\n",
      "50%     0.000000    0.000000   0.000000   0.000000   0.000000\n",
      "75%     0.000000    0.000000   0.000000   0.000000  10.717500\n",
      "max    20.000000   40.000000  50.000000  50.000000  50.000000\n",
      "\n",
      "Average of All column:  8.7\n",
      "Precision:  0.5300739137853476\n",
      "Recall:  0.12643678160919541\n",
      "F1: 0.0531570919501954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:994: UserWarning: unknown class(es) ['Architecture', 'Crime', 'Dance', 'Earth', 'Education', 'Exercise', 'Fitness', 'Food', 'Internet', 'Media', 'Painting', 'Recreation'] will be ignored\n",
      "  warnings.warn('unknown class(es) {0} will be ignored'\n"
     ]
    }
   ],
   "source": [
    "with open('../data/test_30_tested.json') as json_file:\n",
    "    articles_test = json.load(json_file)\n",
    "    \n",
    "titles = [art.get('title') for art in articles_test]\n",
    "anchors = [0] * 30\n",
    "categories = [0] * 30\n",
    "infoboxes = [0] * 30\n",
    "texts = [0] * 30\n",
    "extended = [0] * 30\n",
    "counter = 0\n",
    "\n",
    "for art in articles_test:\n",
    "    len_annotated = len(art.get('annotated_categories'))\n",
    "\n",
    "    n_anchors = take(len_annotated, art.get('anchor_sims_text').keys())\n",
    "    n_anchors.extend(take(len_annotated, art.get('categories_sims_text').keys()))\n",
    "    n_anchors.extend(take(len_annotated, art.get('infobox_sims_text').keys()))\n",
    "    n_anchors.extend(take(len_annotated, art.get('text_sims_text').keys()))\n",
    "    art['extended_sims_text'] = list(set(n_anchors))\n",
    "\n",
    "    for cat in art.get('annotated_categories'):\n",
    "        if cat in art.get('anchor_sims_text') and not pd.isna(art.get('anchor_sims_text').get(cat)) and list(art.get('anchor_sims_text')).index(cat) + 1 <= len_annotated:\n",
    "            anchors[counter] += 1\n",
    "        if cat in art.get('categories_sims_text') and not pd.isna(art.get('categories_sims_text').get(cat)) and list(art.get('categories_sims_text')).index(cat) + 1 <= len_annotated:\n",
    "            categories[counter] += 1\n",
    "        if cat in art.get('infobox_sims_text') and not pd.isna(art.get('infobox_sims_text').get(cat)) and list(art.get('infobox_sims_text')).index(cat) + 1 <= len_annotated:\n",
    "            infoboxes[counter] += 1\n",
    "        if cat in art.get('text_sims_text') and not pd.isna(art.get('text_sims_text').get(cat)) and list(art.get('text_sims_text')).index(cat) + 1 <= len_annotated:\n",
    "            texts[counter] += 1\n",
    "        if cat in art.get('extended_sims_text'):\n",
    "            extended[counter] += 1\n",
    "    anchors[counter] = float(\"{:.2f}\".format(anchors[counter] / len_annotated * 100))\n",
    "    categories[counter] = float(\"{:.2f}\".format(categories[counter] / len_annotated * 100))\n",
    "    infoboxes[counter] = float(\"{:.2f}\".format(infoboxes[counter] / len_annotated * 100))\n",
    "    texts[counter] = float(\"{:.2f}\".format(texts[counter] / len_annotated* 100))\n",
    "    extended[counter] = float(\"{:.2f}\".format(extended[counter] / len_annotated* 100))\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "cars = {\n",
    "    'Anchor': anchors,\n",
    "    'Categories': categories,\n",
    "    'Infobox': infoboxes, \n",
    "    'Text': texts,\n",
    "    'All': extended\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars, columns = ['Anchor','Categories','Infobox','Text','All'], index=titles)\n",
    "\n",
    "print(df)\n",
    "print (df.describe())\n",
    "print('\\nAverage of All column: ', float(\"{:.2f}\".format(df['All'].mean())))\n",
    "\n",
    "A=[art['annotated_categories'] for art in articles_test]\n",
    "B=[art['extended_sims_text'] for art in articles_test]\n",
    "\n",
    "multi = MultiLabelBinarizer()\n",
    "\n",
    "y_true = multi.fit(A).transform(A)\n",
    "y_pred = multi.transform(B)\n",
    "\n",
    "print('Precision: ',precision_score(y_true, y_pred,average='weighted',zero_division=1))\n",
    "print('Recall: ',recall_score(y_true, y_pred, average='weighted',zero_division=1))\n",
    "print('F1:' ,f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index - hladanie podla textu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Achilles', 'Alien', 'Academy Awards', 'Animation', 'Alchemy', 'Ayn Rand', 'Alabama', 'Academy Award for Best Production Design', 'Algeria', 'Aristotle', 'An American in Paris', 'Anthropology'}\n"
     ]
    }
   ],
   "source": [
    "i=invertedIndex(articles_test, 'text_tokens')\n",
    "#print(i)\n",
    "\n",
    "print(i.search(\"arts FILM\", \"and\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invertovany index - hladanie podla kategorie (infobox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alien', 'Academy Awards', 'Animation', 'Actrius', 'Academy Award for Best Production Design'}\n"
     ]
    }
   ],
   "source": [
    "j=invertedIndex(articles_test, 'extended_sims_info')\n",
    "#print(j)\n",
    "\n",
    "print(j.search(\"arts FILM\", \"and\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invertovany index - hladanie podla kategorie (DATAMUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Academy Award for Best Production Design'}\n"
     ]
    }
   ],
   "source": [
    "k=invertedIndex(articles_test, 'extended_sims')\n",
    "#print(j)\n",
    "\n",
    "print(k.search(\"arts FILM\", \"and\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testovanie na vsetkych datach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stiahli sme z Wikipedia dumpu XML suboru enwiki-latest-pages-articles.xml vsetky clanky a rozdelili do 207 súborov, pričom každý obsahoval 100 000 záznamov\n",
    "\n",
    "Celym testovanim presla kvoli nedostatku casu a casovo narocnym operacie iba mensia cast clankov, konkrétne 47 týchto súborov si prešlo celým cyklom od prespracovania po testovanie na vytvorených modeloch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spracovanych plnohodnotnych clankov:  2547414\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in range(16,64):\n",
    "    with open(f'../data/wiki_{i}_before.json') as json_file:\n",
    "        more_articles = json.load(json_file)\n",
    "        counter = counter + len(more_articles)\n",
    "print('Spracovanych plnohodnotnych clankov: ',counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presna zhoda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Z tela článku vyhľadať najčastejšie používané termy a tie, ktoré boli identifikované v kroku 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find exact match words or expressions with categorised words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Find exact match words or expressions with categorised words\n",
    "\n",
    "def find_exact_match(articles, categories):\n",
    "    for article in articles:\n",
    "        article['categories_exact_text'] = []\n",
    "        article['categories_exact_anchors'] = []\n",
    "        article['categories_exact_infobox'] = []\n",
    "        for category in categories:\n",
    "            related_words = category.get('related_words')\n",
    "            found_text = []\n",
    "            found_anchors = []\n",
    "            found_infobox = []\n",
    "            found_text = list(filter(lambda word: re.findall(rf'\\W+({word})\\W+', article['text'], re.IGNORECASE), related_words))\n",
    "            found_anchors = list(filter(lambda word: re.findall(rf'\\W+({word})\\W+', str(article['anchors']).strip('[]'), re.IGNORECASE), related_words))\n",
    "            found_infobox = list(filter(lambda word: re.findall(rf'\\W+({word})\\W+', str(article['infobox']).strip('[]'), re.IGNORECASE), related_words))\n",
    "            if found_text:\n",
    "                article['categories_exact_text'].append({'category':category.get('category'),'related_words':found_text})\n",
    "            if found_anchors:\n",
    "                article['categories_exact_anchors'].append({'category':category.get('category'),'related_words':found_anchors})\n",
    "            if found_infobox:\n",
    "                article['categories_exact_infobox'].append({'category':category.get('category'),'related_words':found_infobox})\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles(articles, file_name):\n",
    "    with open(f'../data/{file_name}.json', 'w') as outfile:\n",
    "        json.dump(articles, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exact_match = find_exact_match(articles, cats_with_words)\n",
    "save_articles(articles_test, 'test_30testedd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vyskusat PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "conf = SparkConf().setAppName('SparkApp').setMaster(\"local\")\n",
    "sc = pyspark.SparkContext(conf = conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "numeric_val = sc.parallelize(articles)\n",
    "square_udf_int = udf(lambda z: remove_stop_words(z))\n",
    "#numeric_val.map(lambda x: remove_stop_words(x)).collect()\n",
    "toc = time.perf_counter()\n",
    "print(f\"Performed in {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "numeric_val.map(lambda x: square_udf_int(x)).collect()\n",
    "toc = time.perf_counter()\n",
    "print(f\"Performed in {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "square_udf_int = udf(lambda z: square(z), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_udf_int([1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
