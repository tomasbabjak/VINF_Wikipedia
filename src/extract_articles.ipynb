{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zhlukovanie článkov Wikipédie do kategórií na základe ich vedecko-spoločenskej oblasti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vypracoval:** Tomáš Babjak\n",
    "\n",
    "**Predmet:** Vyhľadávanie informácii\n",
    "\n",
    "**GitHub:** https://github.com/tomasbabjak/VINF_Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "import re\n",
    "import datamuse\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vytvoriť testovaciu vzorku dát, na ktorej budeme prvotne projekt realizovať"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read XML file with Wiki articles and parse articles to list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(file_name, n_first_articles):\n",
    "    \n",
    "    start_tag = f'<page>'\n",
    "    end_tag = f'</page>'\n",
    "    \n",
    "    start_found = False\n",
    "    articles_found = []\n",
    "    lines = ''\n",
    "    \n",
    "    with open(file_name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if start_tag in line:\n",
    "                start_found = True\n",
    "            if start_found:\n",
    "                lines += line\n",
    "            if end_tag in line:\n",
    "                start_found = False\n",
    "                articles_found.append(lines)\n",
    "                lines = ''\n",
    "            # treba vyfilterovat <title> a <text>, mozno aj <id> pre indexaciu\n",
    "            if len(articles_found) == n_first_articles:\n",
    "                break\n",
    "    with open(f'../data/wiki_{n_first_articles}_before.json', 'w') as outfile:\n",
    "        json.dump(articles_found, outfile)\n",
    "    return articles_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Title and Text attributes from article and create dictionary from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(text):\n",
    "    title_regex = r'<title[^>]*>([^<]+)</title>'\n",
    "    text_regex = r'<text[^>]*>([^<]+)</text>'\n",
    "    pages = []\n",
    "    for page in text:\n",
    "        title = regex.findall(title_regex, page)\n",
    "        text = regex.findall(text_regex, page)\n",
    "        pages.append({\"title\": title[0] if title else '',\n",
    "                      \"text\": text[0] if text else ''})\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Z článkov testovacej sady vyhľadať dôležité pojmy - zamerať sa na Infobox, kde sa nachádzajú dôležité informácie o článku\n",
    "\n",
    "### 5. Vyhľadať odkazy na iné články Wikipédie (anchor text), ktoré môžu smerovať priamo na oblasť alebo aspoň priblížiť kontext článku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Infobox and Achor texts from Text attribute of article and add them to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_infobox_anchor(text):\n",
    "    regex_infobox = r\"(?=\\{Infobox )(\\{([^{}]|(?1))*\\})\"\n",
    "    regex_anchor = r\"\\[\\[([^\\]\\[:]+)\\|([^\\]\\[:]+)\\]\\]\"\n",
    "    for page in text:\n",
    "        page['infobox'] = regex.findall(regex_infobox, page['text'])\n",
    "        page['anchors'] = regex.findall(regex_anchor, page['text'])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate Redirect articles from others into two separate lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_redirect(text):\n",
    "    regex_redirect = r\"^#REDIRECT[^\\[]*\\[\\[([^\\]]+)\"\n",
    "    redirect_pages = []\n",
    "    article_pages = []\n",
    "    for page in text:\n",
    "        if regex.findall(regex_redirect, page['text']):\n",
    "            redirect_pages.append(page)\n",
    "        else:\n",
    "            article_pages.append(page)\n",
    "    return (redirect_pages, article_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects, articles = find_redirect(find_infobox_anchor(extract_text(read_xml('../data/enwiki-latest-pages-articles.xml', 100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vytvoriť zoznam (strom) spoločensko-vedných oblastí, do ktorých budeme jednotlivé stránky zaraďovať, ku každej oblasti nájsť aj slová, ktoré sa s ňou spájajú"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find terms related to our categories with Datamuse library. Split words of each category and find 100 terms related to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api = datamuse.Datamuse()\n",
    "\n",
    "def categories_find_related():\n",
    "    \n",
    "    categories = [\n",
    "        'Culture, literature and the arts',\n",
    "        'Geography - places and states',\n",
    "        'Medicine - health and fitness',\n",
    "        'History and events',\n",
    "        'Mathematics and logic',\n",
    "        'Nature and physics',\n",
    "        'Technology and computing',\n",
    "        'Philosophy and thinking',\n",
    "        'Religion and belief',\n",
    "        'Society, politics and people'\n",
    "    ]\n",
    "    cats_with_words = []\n",
    "    \n",
    "    for c in categories:\n",
    "        keywords = regex.split(' - |, | and ',c)\n",
    "        num = 11 if len(keywords) == 2 else 7\n",
    "        api_words = []\n",
    "        for word in keywords:\n",
    "            api_words.extend(api.words(ml=word, max=num))\n",
    "        result = list(map(lambda x: x.get('word'), api_words))\n",
    "        result.extend(list(map(lambda x: x.lower() ,keywords)))\n",
    "        cats_with_words.append({'category':c,'related_words':result})\n",
    "    return cats_with_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cats_with_words = categories_find_related()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Články vhodne predspracovať - stemming, tokenizácia, odstránenie stop slov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_articles(articles):\n",
    "    for article in articles:\n",
    "        nltk_tokens = nltk.word_tokenize(article.get('text'))\n",
    "        print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '{short description|Political philosophy and movement}}\\n{{redirect2|Anarchist|Anarchists|other uses|'\n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "grams_2 = nltk.ngrams('Toto je moj super text.'.split(), 2)\n",
    "\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom word tokenizer\n",
    "def tokzr_WORD(txt): \n",
    "    return ('WORD', re.findall(r'(?ms)\\W*(\\w+)', txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Z tela článku vyhľadať najčastejšie používané termy a tie, ktoré boli identifikované v kroku 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find exact match words or expressions with categorised words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_exact_match(articles, categories):\n",
    "    for article in articles:\n",
    "        article['categories_exact_text'] = []\n",
    "        article['categories_exact_anchors'] = []\n",
    "        article['categories_exact_infobox'] = []\n",
    "        for category in categories:\n",
    "            related_words = category.get('related_words')\n",
    "            found_text = []\n",
    "            found_anchors = []\n",
    "            found_infobox = []\n",
    "            found_text = list(filter(lambda word: re.findall(rf'\\W+({word})\\W+', article['text'], re.IGNORECASE), related_words))\n",
    "            found_anchors = list(filter(lambda word: re.findall(rf'\\W+({word})\\W+', str(article['anchors']).strip('[]'), re.IGNORECASE), related_words))\n",
    "            found_infobox = list(filter(lambda word: re.findall(rf'\\W+({word})\\W+', str(article['infobox']).strip('[]'), re.IGNORECASE), related_words))\n",
    "            if found_text:\n",
    "                article['categories_exact_text'].append({'category':category.get('category'),'related_words':found_text})\n",
    "            if found_anchors:\n",
    "                article['categories_exact_anchors'].append({'category':category.get('category'),'related_words':found_anchors})\n",
    "            if found_infobox:\n",
    "                article['categories_exact_infobox'].append({'category':category.get('category'),'related_words':found_infobox})\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles(articles, file_name):\n",
    "    with open(f'../data/{file_name}.json', 'w') as outfile:\n",
    "        json.dump(articles, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match = find_exact_match(articles, cats_with_words)\n",
    "save_articles(exact_match, 'wiki_100_exact_match')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
